{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b98a5ae",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection – Supervised Learning Project\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "Credit card fraud causes massive financial losses each year. The goal of this project is to build a **supervised machine learning model** that can detect fraudulent transactions based on anonymized transaction features.\n",
    "\n",
    "We will use the **Credit Card Fraud Detection** dataset (European cardholders, September 2013), which is a **highly imbalanced binary classification problem**:\n",
    "\n",
    "- **Class 0** – legitimate transaction  \n",
    "- **Class 1** – fraudulent transaction  \n",
    "\n",
    "Our objectives:\n",
    "\n",
    "- Perform **Exploratory Data Analysis (EDA)** to understand the data distribution and class imbalance.\n",
    "- Build several supervised ML models (e.g., Logistic Regression, Random Forest).\n",
    "- Handle **severe class imbalance** using techniques like class weighting (and optionally resampling).\n",
    "- Evaluate models using appropriate metrics (Precision, Recall, F1, ROC-AUC, PR curve).\n",
    "- Compare models and discuss trade-offs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377d03a",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75998f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup & Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load dataset\n",
    "\n",
    "# Adjust this path if needed\n",
    "DATA_PATH = \"data/creditcard.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Basic info and summary\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac88ce",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Description\n",
    "\n",
    "- Number of rows (transactions): `df.shape[0]`  \n",
    "- Number of columns (features + target): `df.shape[1]`  \n",
    "- Features:\n",
    "  - `Time` – seconds elapsed between this transaction and the first transaction in the dataset  \n",
    "  - `Amount` – transaction amount  \n",
    "  - `V1` … `V28` – numeric features obtained by PCA transformation (anonymized)  \n",
    "- Target:\n",
    "  - `Class` – 0 for legitimate transactions, 1 for fraudulent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a67a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Class distribution\n",
    "\n",
    "class_counts = df[\"Class\"].value_counts()\n",
    "class_ratio = df[\"Class\"].value_counts(normalize=True)\n",
    "\n",
    "print(\"Class counts:\\n\", class_counts)\n",
    "print(\"\\nClass ratio:\\n\", class_ratio)\n",
    "\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.title(\"Class Distribution (0 = Non-fraud, 1 = Fraud)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f827694",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "- The dataset is **heavily imbalanced**.\n",
    "- Fraud (Class = 1) represents a very small fraction of transactions (typically ~0.17%).\n",
    "- This has important implications for modeling and evaluation:\n",
    "  - Accuracy alone is misleading.\n",
    "  - We must pay attention to **precision, recall, F1-score**, and **ROC/PR curves**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Distribution of 'Amount'\n",
    "\n",
    "sns.histplot(df[\"Amount\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Transaction Amount\")\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Distribution of 'Time'\n",
    "\n",
    "sns.histplot(df[\"Time\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Time (seconds from first transaction)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Amount vs Class\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"Amount\", data=df)\n",
    "plt.title(\"Transaction Amount by Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e18e8",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Many transactions have relatively small amounts, with a long tail of larger transactions.\n",
    "- Fraudulent transactions may have different amount distributions (we can comment based on plots).\n",
    "- Time might exhibit some patterns, but it's not always strongly predictive on its own.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81127438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Correlation matrix (features only, excluding 'Class')\n",
    "\n",
    "corr = df.drop(columns=[\"Class\"]).corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfba178",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- The `V` features are PCA components, so they are decorrelated in specific ways.\n",
    "- `Amount` and `Time` are original features and may show weaker correlations.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Preparation & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Check for missing values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f6075",
   "metadata": {},
   "source": [
    "Typically, the Kaggle credit card dataset has **no missing values**, but this check confirms.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Train–Test Split (with Stratification)\n",
    "\n",
    "We will:\n",
    "- Use `Class` as the target variable.\n",
    "- Use all other columns as features.\n",
    "- Perform a stratified split to preserve the fraud ratio in both train and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Class\", axis=1)\n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f317ae",
   "metadata": {},
   "source": [
    "### 4.3 Scaling Features\n",
    "\n",
    "- PCA features (`V1`–`V28`) are already scaled.\n",
    "- `Amount` and `Time` are not.\n",
    "- We will still apply `StandardScaler` inside a `Pipeline` so all numeric features are scaled consistently for models like Logistic Regression.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Quick baseline: majority class accuracy for reference\n",
    "\n",
    "majority_class = y_train.mode()[0]\n",
    "baseline_pred = np.full_like(y_test, fill_value=majority_class)\n",
    "\n",
    "print(\"Baseline accuracy (predict all 0):\", np.mean(baseline_pred == y_test))\n",
    "print(\"Fraud rate in test set:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08605bbc",
   "metadata": {},
   "source": [
    "The baseline accuracy will be very high simply by predicting **non-fraud (0)** all the time, but this ignores fraudulent cases.  \n",
    "This shows **why we must not rely solely on accuracy** in imbalanced settings.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Modeling & Evaluation\n",
    "\n",
    "We will start with:\n",
    "\n",
    "1. Logistic Regression (with class weights)\n",
    "2. Random Forest (with class weights)\n",
    "3. Optional: Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "We will use:\n",
    "- Confusion matrix\n",
    "- Precision, recall, F1-score\n",
    "- ROC-AUC\n",
    "- Precision–Recall curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e758e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    PrecisionRecallDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b21c1f",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression (with class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Logistic Regression Pipeline\n",
    "\n",
    "log_reg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a51d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression\n",
    "\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "y_proba_lr = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Logistic Regression (class_weight='balanced') ===\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=4))\n",
    "\n",
    "roc_auc_lr = roc_auc_score(y_test, y_proba_lr)\n",
    "print(\"ROC-AUC:\", roc_auc_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for Logistic Regression\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f\"LogReg (AUC = {roc_auc_lr:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f588a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve for Logistic Regression\n",
    "\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_proba_lr)\n",
    "ap_lr = average_precision_score(y_test, y_proba_lr)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(recall_lr, precision_lr, label=f\"LogReg (AP = {ap_lr:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve – Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c1e76",
   "metadata": {},
   "source": [
    "### 5.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Random Forest Classifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\"\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fad589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Random Forest (class_weight='balanced_subsample') ===\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, digits=4))\n",
    "\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "print(\"ROC-AUC:\", roc_auc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve – Random Forest vs Logistic Regression\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f\"LogReg (AUC = {roc_auc_lr:.3f})\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (AUC = {roc_auc_rf:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – Model Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve – Random Forest vs Logistic Regression\n",
    "\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_proba_rf)\n",
    "ap_rf = average_precision_score(y_test, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(recall_lr, precision_lr, label=f\"LogReg (AP = {ap_lr:.3f})\")\n",
    "plt.plot(recall_rf, precision_rf, label=f\"Random Forest (AP = {ap_rf:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve – Model Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca44f5",
   "metadata": {},
   "source": [
    "### 5.3 Hyperparameter Tuning (Example: Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Hyperparameter Tuning for Logistic Regression\n",
    "\n",
    "param_grid_lr = {\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10],\n",
    "    \"clf__penalty\": [\"l2\"]\n",
    "}\n",
    "\n",
    "grid_log_reg = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae40dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters (LogReg):\", grid_log_reg.best_params_)\n",
    "print(\"Best CV F1-score:\", grid_log_reg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Logistic Regression on test set\n",
    "\n",
    "best_lr = grid_log_reg.best_estimator_\n",
    "\n",
    "y_pred_best_lr = best_lr.predict(X_test)\n",
    "y_proba_best_lr = best_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Tuned Logistic Regression ===\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_best_lr))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best_lr, digits=4))\n",
    "\n",
    "roc_auc_best_lr = roc_auc_score(y_test, y_proba_best_lr)\n",
    "print(\"ROC-AUC:\", roc_auc_best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e283943",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = OrderedDict()\n",
    "\n",
    "results[\"LogReg (balanced)\"] = {\n",
    "    \"ROC-AUC\": roc_auc_lr,\n",
    "    \"AP\": ap_lr\n",
    "}\n",
    "results[\"Random Forest (balanced_subsample)\"] = {\n",
    "    \"ROC-AUC\": roc_auc_rf,\n",
    "    \"AP\": ap_rf\n",
    "}\n",
    "results[\"Tuned LogReg\"] = {\n",
    "    \"ROC-AUC\": roc_auc_best_lr,\n",
    "    \"AP\": average_precision_score(y_test, y_proba_best_lr)\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9e419",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "- Compare models by **ROC-AUC** and **Average Precision (AP)**.\n",
    "- Discuss:\n",
    "  - Which model has better recall on fraud cases?\n",
    "  - Which model has higher precision?\n",
    "  - Is there a trade-off? Which model is preferable in a real-world fraud setting?\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion & Future Work\n",
    "\n",
    "### 7.1 Summary\n",
    "\n",
    "- We tackled a **highly imbalanced binary classification** problem: credit card fraud detection.\n",
    "- We performed EDA:\n",
    "  - Verified strong class imbalance.\n",
    "  - Inspected distributions of Time and Amount.\n",
    "  - Looked at correlations between features.\n",
    "- We built and evaluated multiple models:\n",
    "  - Logistic Regression with class weighting.\n",
    "  - Random Forest with class weighting.\n",
    "  - Tuned Logistic Regression with GridSearchCV.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- Class imbalance makes **accuracy misleading**.\n",
    "- Models must be judged using **precision, recall, F1-score, ROC-AUC, and PR curves**.\n",
    "- Random Forest often provides strong performance out-of-the-box, while Logistic Regression is more interpretable.\n",
    "\n",
    "### 7.2 Limitations\n",
    "\n",
    "- Features `V1`–`V28` are anonymized PCA components → limited interpretability.\n",
    "- Dataset represents a specific time window and region; generalization to other domains may require retraining.\n",
    "\n",
    "### 7.3 Future Work\n",
    "\n",
    "- Experiment with:\n",
    "  - **SMOTE** or other advanced resampling techniques.\n",
    "  - More advanced models (e.g., XGBoost, LightGBM).\n",
    "  - Cost-sensitive learning (different misclassification costs for fraud vs non-fraud).\n",
    "- Calibrate predicted probabilities (e.g., via Platt scaling or isotonic regression).\n",
    "- Deploy as an API or web app for real-time fraud scoring.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. References\n",
    "\n",
    "- Kaggle – Credit Card Fraud Detection dataset  \n",
    "- Scikit-learn documentation  \n",
    "- Imbalanced classification resources\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
